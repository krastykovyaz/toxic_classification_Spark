{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-04T16:30:33.082851Z","iopub.execute_input":"2022-12-04T16:30:33.083586Z","iopub.status.idle":"2022-12-04T16:30:33.113843Z","shell.execute_reply.started":"2022-12-04T16:30:33.083482Z","shell.execute_reply":"2022-12-04T16:30:33.113171Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test_labels.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\n/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Set configurations and libraries","metadata":{}},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:30:35.944870Z","iopub.execute_input":"2022-12-04T16:30:35.945303Z","iopub.status.idle":"2022-12-04T16:31:14.018110Z","shell.execute_reply.started":"2022-12-04T16:30:35.945272Z","shell.execute_reply":"2022-12-04T16:31:14.016583Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=18a114435ac5ea0192dca177a95d35676190ebd331982cc58dd1c2852e3ccbdf\n  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\nfrom pyspark.ml.feature import Tokenizer, HashingTF, IDF\nfrom pyspark.ml.classification import LogisticRegression","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:31:14.020269Z","iopub.execute_input":"2022-12-04T16:31:14.020701Z","iopub.status.idle":"2022-12-04T16:31:14.150233Z","shell.execute_reply.started":"2022-12-04T16:31:14.020660Z","shell.execute_reply":"2022-12-04T16:31:14.149245Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"configs = (SparkSession.builder\n                  .appName('Toxic Comment Classification')\n                  .enableHiveSupport()\n                  .config(\"spark.executor.memory\", \"4G\")\n                  .config(\"spark.driver.memory\",\"18G\")\n                  .config(\"spark.executor.cores\",\"7\")\n                  .config(\"spark.python.worker.memory\",\"4G\")\n                  .config(\"spark.driver.maxResultSize\",\"0\")\n                  .config(\"spark.sql.crossJoin.enabled\", \"true\")\n                  .config(\"spark.serializer\",\"org.apache.spark.serializer.KryoSerializer\")\n                  .config(\"spark.default.parallelism\",\"2\")\n                  .getOrCreate())\n\nconfigs.sparkContext.setLogLevel('ERROR')","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:31:14.151673Z","iopub.execute_input":"2022-12-04T16:31:14.151972Z","iopub.status.idle":"2022-12-04T16:31:18.908244Z","shell.execute_reply.started":"2022-12-04T16:31:14.151945Z","shell.execute_reply":"2022-12-04T16:31:18.907439Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","output_type":"stream"},{"name":"stdout","text":"22/12/04 16:31:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Read dataframe to spark","metadata":{}},{"cell_type":"code","source":"pd_test_answear = pd.DataFrame()\ndef to_spark_df(fin):\n    df = pd.read_csv(fin)\n    df.fillna(\"\", inplace=True)\n    df_train = df.sample(frac=0.7)\n    df_test = df.loc[~df.index.isin(df_train.index)]\n    global pd_test_answear\n    pd_test_answear = df_test.iloc[:,2:].copy()\n    df_train = configs.createDataFrame(df_train)\n    df_test = configs.createDataFrame(df_test.iloc[:,:2])\n    return(df_train, df_test)\n\ndef convert_to_spark_df(fin):\n    df = pd.read_csv(fin)\n    df.fillna(\"\", inplace=True)\n    df = configs.createDataFrame(df)\n    return df\n\n\ntrain, test = to_spark_df(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest_o = convert_to_spark_df(\"/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:31:18.912133Z","iopub.execute_input":"2022-12-04T16:31:18.912428Z","iopub.status.idle":"2022-12-04T16:31:38.935402Z","shell.execute_reply.started":"2022-12-04T16:31:18.912402Z","shell.execute_reply":"2022-12-04T16:31:38.934017Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### TFIDF tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")\nwordsData = tokenizer.transform(train)\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=1000)\ntf = hashingTF.transform(wordsData)\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(tf) \ntfidf = idfModel.transform(tf)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:46:45.223080Z","iopub.execute_input":"2022-12-04T15:46:45.223549Z","iopub.status.idle":"2022-12-04T15:46:49.682778Z","shell.execute_reply.started":"2022-12-04T15:46:45.223511Z","shell.execute_reply":"2022-12-04T15:46:49.681809Z"},"trusted":true},"execution_count":178,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Logistic regression","metadata":{}},{"cell_type":"code","source":"lr = LogisticRegression(featuresCol=\"features\", labelCol='toxic', regParam=0.1)\nlrModel = lr.fit(tfidf)\nres_train = lrModel.transform(tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:46:49.684538Z","iopub.execute_input":"2022-12-04T15:46:49.684819Z","iopub.status.idle":"2022-12-04T15:47:00.554502Z","shell.execute_reply.started":"2022-12-04T15:46:49.684793Z","shell.execute_reply":"2022-12-04T15:47:00.553114Z"},"trusted":true},"execution_count":179,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"test_tokens = tokenizer.transform(test)\ntest_tf = hashingTF.transform(test_tokens)\ntest_tfidf = idfModel.transform(test_tf)\ntest_res_log = test.select('id')\nextract_prob = F.udf(lambda x: float(x[1]), T.FloatType())\n# extract_prob = F.map(f)\nfocus_cols = list(filter(lambda x: x not in [\"id\", \"comment_text\"], train.columns))\nfor col in focus_cols:\n    lr = LogisticRegression(featuresCol=\"features\", labelCol=col, regParam=0.1)\n    lrModel = lr.fit(tfidf)\n    res = lrModel.transform(test_tfidf)\n    test_res_log = test_res_log.join(res.select('id', 'probability'), on=\"id\")\n    test_res_log = test_res_log.withColumn(col, extract_prob('probability')).drop(\"probability\")\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:47:00.556115Z","iopub.execute_input":"2022-12-04T15:47:00.556473Z","iopub.status.idle":"2022-12-04T15:48:06.353908Z","shell.execute_reply.started":"2022-12-04T15:47:00.556447Z","shell.execute_reply":"2022-12-04T15:48:06.352620Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Linear regression","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.regression import LinearRegression\nlr = LinearRegression(featuresCol=\"features\", labelCol='toxic', regParam=0.1)\nlrModel = lr.fit(tfidf)\nres_train = lrModel.transform(tfidf)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:48:52.196374Z","iopub.execute_input":"2022-12-04T15:48:52.196860Z","iopub.status.idle":"2022-12-04T15:49:02.285100Z","shell.execute_reply.started":"2022-12-04T15:48:52.196824Z","shell.execute_reply":"2022-12-04T15:49:02.283192Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"test_tokens = tokenizer.transform(test)\ntest_tf = hashingTF.transform(test_tokens)\ntest_tfidf = idfModel.transform(test_tf)\ntest_res_lin = test.select('id')\nextract_prob = F.udf(lambda x: float(x[1]), T.FloatType())\nfocus_cols = list(filter(lambda x: x not in [\"id\", \"comment_text\"], train.columns))\nfor col in focus_cols:\n    lr = LinearRegression(featuresCol=\"features\", labelCol=col, regParam=0.1)\n    lrModel = lr.fit(tfidf)\n    res = lrModel.transform(test_tfidf)\n    test_res_lin = test_res_lin.join(res.select('id', 'prediction'), on=\"id\")\n    test_res_lin = test_res_lin.withColumnRenamed('prediction', col)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:49:02.286477Z","iopub.execute_input":"2022-12-04T15:49:02.286754Z","iopub.status.idle":"2022-12-04T15:50:01.488600Z","shell.execute_reply.started":"2022-12-04T15:49:02.286729Z","shell.execute_reply":"2022-12-04T15:50:01.486266Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Metrics TfIdf","metadata":{"execution":{"iopub.status.busy":"2022-12-04T13:39:39.329534Z","iopub.execute_input":"2022-12-04T13:39:39.329945Z","iopub.status.idle":"2022-12-04T13:39:39.852349Z","shell.execute_reply.started":"2022-12-04T13:39:39.329910Z","shell.execute_reply":"2022-12-04T13:39:39.851178Z"}}},{"cell_type":"markdown","source":"numFeatures=20000","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ny_true = pd_test_answear.toxic\nf = lambda x: 1 if x > 0.5 else 0\ndf_test_log = test_res_log.toPandas()\nprint('Logistic accuracy', accuracy_score(y_true, df_test_log.toxic.map(f)))\ndf_test_lin = test_res_lin.toPandas()\nprint('Linear accuracy', accuracy_score(y_true, df_test_lin.toxic.map(f)))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:36:13.261095Z","iopub.execute_input":"2022-12-04T15:36:13.261599Z","iopub.status.idle":"2022-12-04T15:36:17.051506Z","shell.execute_reply.started":"2022-12-04T15:36:13.261561Z","shell.execute_reply":"2022-12-04T15:36:17.049060Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Logistic accuracy 0.8907689415303629\nLinear accuracy 0.8902049257379206\n","output_type":"stream"}]},{"cell_type":"markdown","source":"numFeatures=1000","metadata":{}},{"cell_type":"code","source":"df_test_log = test_res_log.toPandas()\nprint('Logistic accuracy', accuracy_score(y_true, df_test_log.toxic.map(f)))\ndf_test_lin = test_res_lin.toPandas()\nprint('Linear accuracy', accuracy_score(y_true, df_test_lin.toxic.map(f)))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:50:01.490698Z","iopub.execute_input":"2022-12-04T15:50:01.491457Z","iopub.status.idle":"2022-12-04T15:50:27.510180Z","shell.execute_reply.started":"2022-12-04T15:50:01.491379Z","shell.execute_reply":"2022-12-04T15:50:27.508025Z"},"trusted":true},"execution_count":184,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Logistic accuracy 0.9020701468530008\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"Linear accuracy 0.9021119258005891\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Given the nature of the algorithm, if numFeatures is less than the actual number of distinct words/tokens in the DataFrame you are guaranteed to have an 'incorrect' frequency for at least 1 token (i.e. different tokens will hash to the same bucket). Even with numFeatures >= vocabularySize collisions 'might' still happen.","metadata":{"execution":{"iopub.status.busy":"2022-12-04T15:22:10.476077Z","iopub.execute_input":"2022-12-04T15:22:10.476534Z","iopub.status.idle":"2022-12-04T15:22:13.477183Z","shell.execute_reply.started":"2022-12-04T15:22:10.476499Z","shell.execute_reply":"2022-12-04T15:22:13.475500Z"}}},{"cell_type":"markdown","source":"### Word2vec approach","metadata":{}},{"cell_type":"code","source":"from pyspark.ml.feature import Word2Vec","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:31:38.936825Z","iopub.execute_input":"2022-12-04T16:31:38.937247Z","iopub.status.idle":"2022-12-04T16:31:38.943325Z","shell.execute_reply.started":"2022-12-04T16:31:38.937213Z","shell.execute_reply":"2022-12-04T16:31:38.942476Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(inputCol=\"comment_text\", outputCol=\"words\")\nwordsData = tokenizer.transform(train)\nw2v_tokenizer = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"rawFeatures\")\nw2v = w2v_tokenizer.fit(wordsData)\nw2v_df = w2v.transform(wordsData)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:31:38.944426Z","iopub.execute_input":"2022-12-04T16:31:38.945507Z","iopub.status.idle":"2022-12-04T16:34:31.099542Z","shell.execute_reply.started":"2022-12-04T16:31:38.945426Z","shell.execute_reply":"2022-12-04T16:34:31.098786Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"lr = LogisticRegression(featuresCol=\"rawFeatures\", labelCol='toxic', regParam=0.1)\nlrModel = lr.fit(w2v_df)\nres_train = lrModel.transform(w2v_df)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:34:31.101629Z","iopub.execute_input":"2022-12-04T16:34:31.102312Z","iopub.status.idle":"2022-12-04T16:34:39.899435Z","shell.execute_reply.started":"2022-12-04T16:34:31.102269Z","shell.execute_reply":"2022-12-04T16:34:39.898625Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"code","source":"test_tokens = tokenizer.transform(test)\ntest_w2v = w2v.transform(test_tokens)\ntest_res_log = test.select('id')\nfocus_cols = list(filter(lambda x: x not in [\"id\", \"comment_text\"], train.columns))\nfor col in focus_cols:\n    lr = LogisticRegression(featuresCol=\"rawFeatures\", labelCol=col, regParam=0.1)\n    lrModel = lr.fit(w2v_df)\n    res = lrModel.transform(test_w2v)\n    test_res_log = test_res_log.join(res.select('id', 'prediction'), on=\"id\")\n    test_res_log = test_res_log.withColumnRenamed('prediction', col)","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:38:32.861001Z","iopub.execute_input":"2022-12-04T16:38:32.861330Z","iopub.status.idle":"2022-12-04T16:39:17.088852Z","shell.execute_reply.started":"2022-12-04T16:38:32.861306Z","shell.execute_reply":"2022-12-04T16:39:17.087932Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"### Metrics Word2Vec","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:22:40.900675Z","iopub.execute_input":"2022-12-04T16:22:40.901147Z","iopub.status.idle":"2022-12-04T16:22:41.000245Z","shell.execute_reply.started":"2022-12-04T16:22:40.901091Z","shell.execute_reply":"2022-12-04T16:22:40.999366Z"}}},{"cell_type":"code","source":"df_test_log_w2v = test_res_log.toPandas()\nprint('Logistic accuracy w2v', accuracy_score(y_true, df_test_log_w2v.toxic.map(f)))","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:39:43.288859Z","iopub.execute_input":"2022-12-04T16:39:43.289199Z","iopub.status.idle":"2022-12-04T16:39:43.830067Z","shell.execute_reply.started":"2022-12-04T16:39:43.289175Z","shell.execute_reply":"2022-12-04T16:39:43.829159Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Logistic accuracy w2v 0.9038666415992981\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-12-04T16:39:39.905667Z","iopub.execute_input":"2022-12-04T16:39:39.906052Z","iopub.status.idle":"2022-12-04T16:39:40.239154Z","shell.execute_reply.started":"2022-12-04T16:39:39.906019Z","shell.execute_reply":"2022-12-04T16:39:40.237531Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}